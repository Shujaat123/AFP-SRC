{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepSRC",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/AFP-SRC/blob/master/DeepSRC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tcc6qiBvH3A"
      },
      "source": [
        "import sys, os, re, gc\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from random import sample\r\n",
        "\r\n",
        "## Models\r\n",
        "import tensorflow as tf\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input, Dense, BatchNormalization, Dropout\r\n",
        "from keras import metrics\r\n",
        "from keras import optimizers\r\n",
        "from keras.utils.np_utils import to_categorical\r\n",
        "\r\n",
        "## Perfmetrics\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, balanced_accuracy_score, precision_recall_fscore_support\r\n",
        "from sklearn.metrics import auc, average_precision_score, precision_recall_curve, roc_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO4GsEwi6UZp"
      },
      "source": [
        "def SRC_Pred(y_latent_pred):\r\n",
        "    y_pred=[]\r\n",
        "    for i in range(y_latent_pred.shape[1]):\r\n",
        "      \r\n",
        "        if (LA.norm(y_latent_pred[i][y_train==1])>= LA.norm(y_latent_pred[i][y_train==0])):\r\n",
        "            y_pred.append(1)\r\n",
        "        else:\r\n",
        "            y_pred.append(0)\r\n",
        "      \r\n",
        "    return np.array(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydW7vLSY4v6Q"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "class Sampling(layers.Layer):\r\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        z_mean, z_log_var = inputs\r\n",
        "        batch = tf.shape(z_mean)[0]\r\n",
        "        dim = tf.shape(z_mean)[1]\r\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d7VDajGydiX"
      },
      "source": [
        "## Designing an Auto-Encoder-Classifier model\r\n",
        "def Deep_SRC(input_shape=840, LV=600):\r\n",
        "    # Encoder Network\r\n",
        "    enc_input = Input(shape=(input_shape,), name='enc_input')\r\n",
        "    enc_l1 = Dense(100, activation='relu', name='encoder_layer1')(enc_input)\r\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\r\n",
        "    enc_l1 = Dropout(rate = 0.3)(enc_l1)\r\n",
        "\r\n",
        "    enc_l2 = Dense(100, activation='relu', name='encoder_layer2')(enc_l1)\r\n",
        "    enc_l2 = BatchNormalization()(enc_l2)\r\n",
        "    enc_l2 = Dropout(rate = 0.3)(enc_l2)\r\n",
        "\r\n",
        "    enc_l3 = Dense(100, activation='relu', name='encoder_layer3')(enc_l2)\r\n",
        "    enc_l3 = BatchNormalization()(enc_l3)\r\n",
        "    enc_l3 = Dropout(rate = 0.3)(enc_l3)\r\n",
        "\r\n",
        "    enc_l4 = Dense(100, activation='relu', name='encoder_layer4')(enc_l3)\r\n",
        "    enc_l4 = BatchNormalization()(enc_l4)\r\n",
        "    enc_l4 = Dropout(rate = 0.3)(enc_l4)\r\n",
        "\r\n",
        "    encoder_output = Dense(LV, activation='softmax', name='encoder_output')(enc_l4)    \r\n",
        "\r\n",
        "    # # Classifier Network\r\n",
        "    #class_output = Dense(2, activation='softmax', name='class_layer1')(encoder_output)    \r\n",
        "    \r\n",
        "    # Decoder Network\r\n",
        "    dec_l1 = Dense(100, activation='relu', name='decoder_layer1')(encoder_output)\r\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\r\n",
        "    dec_l1 = Dropout(rate = 0.3)(dec_l1)\r\n",
        "\r\n",
        "    dec_l2 = Dense(100, activation='relu', name='decoder_layer2')(dec_l1)\r\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\r\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\r\n",
        "\r\n",
        "    dec_l3 = Dense(100, activation='relu', name='decoder_layer3')(dec_l2)\r\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\r\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\r\n",
        "\r\n",
        "    dec_l4 = Dense(100, activation='relu', name='decoder_layer4')(dec_l3)\r\n",
        "    dec_l4 = BatchNormalization()(dec_l4)\r\n",
        "    dec_l4 = Dropout(rate = 0.3)(dec_l4)\r\n",
        "\r\n",
        "    decoder_output = Dense(input_shape, activation='sigmoid', name='decoder_output')(dec_l4)\r\n",
        "\r\n",
        "    model = Model(inputs=[enc_input], outputs=[encoder_output, decoder_output])\r\n",
        "\r\n",
        "    # Compiling model\r\n",
        "    model.compile(optimizer='adam',#optimizer='rmsprop',\r\n",
        "                  loss={'encoder_output': 'categorical_crossentropy', 'decoder_output': 'mean_squared_error'},\r\n",
        "                  loss_weights={'encoder_output': 0.001, 'decoder_output': 0.999},\r\n",
        "                  metrics=[metrics.categorical_accuracy])\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ams3z_J54Dq1"
      },
      "source": [
        "## Designing an Auto-Encoder-Classifier model\r\n",
        "def Deep_VAE_SRC(input_shape =840, LV=600):\r\n",
        "    # Encoder Network\r\n",
        "    enc_input = Input(shape=(input_shape,), name='enc_input')\r\n",
        "    enc_l1 = Dense(100, activation='relu', name='encoder_layer1')(enc_input)\r\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\r\n",
        "    enc_l1 = Dropout(rate = 0.3)(enc_l1)\r\n",
        "\r\n",
        "    enc_l2 = Dense(100, activation='relu', name='encoder_layer2')(enc_l1)\r\n",
        "    enc_l2 = BatchNormalization()(enc_l2)\r\n",
        "    enc_l2 = Dropout(rate = 0.3)(enc_l2)\r\n",
        "\r\n",
        "    enc_l3 = Dense(100, activation='relu', name='encoder_layer3')(enc_l2)\r\n",
        "    enc_l3 = BatchNormalization()(enc_l3)\r\n",
        "    enc_l3 = Dropout(rate = 0.3)(enc_l3)\r\n",
        "\r\n",
        "    # enc_l4 = Dense(100, activation='relu', name='encoder_layer4')(enc_l3)\r\n",
        "    # enc_l4 = BatchNormalization()(enc_l4)\r\n",
        "    # enc_l4 = Dropout(rate = 0.3)(enc_l4)\r\n",
        "\r\n",
        "    #encoder_output = Dense(LV, activation='softmax', name='encoder_output')(enc_l4)\r\n",
        "    z_mean = Dense(LV, name=\"z_mean\")(enc_l3)\r\n",
        "    z_log_var = Dense(LV, name=\"z_log_var\")(enc_l3)\r\n",
        "    encoder_output = Sampling()([z_mean, z_log_var])\r\n",
        "    encoder_output = Dense(LV, activation='relu', name='encoder_output')(encoder_output)\r\n",
        "\r\n",
        "    # # Classifier Network\r\n",
        "    #class_output = Dense(2, activation='softmax', name='class_layer1')(encoder_output)    \r\n",
        "    \r\n",
        "    # Decoder Network\r\n",
        "    dec_l1 = Dense(100, activation='relu', name='decoder_layer1')(encoder_output)\r\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\r\n",
        "    dec_l1 = Dropout(rate = 0.3)(dec_l1)\r\n",
        "\r\n",
        "    dec_l2 = Dense(100, activation='relu', name='decoder_layer2')(dec_l1)\r\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\r\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\r\n",
        "\r\n",
        "    dec_l3 = Dense(100, activation='relu', name='decoder_layer3')(dec_l2)\r\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\r\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\r\n",
        "\r\n",
        "    dec_l4 = Dense(100, activation='relu', name='decoder_layer4')(dec_l3)\r\n",
        "    dec_l4 = BatchNormalization()(dec_l4)\r\n",
        "    dec_l4 = Dropout(rate = 0.3)(dec_l4)\r\n",
        "\r\n",
        "    decoder_output = Dense(input_shape, activation='sigmoid', name='decoder_output')(dec_l4)\r\n",
        "\r\n",
        "    model = Model(inputs=[enc_input], outputs=[encoder_output, decoder_output])\r\n",
        "\r\n",
        "    # Compiling model\r\n",
        "    model.compile(optimizer='adam',#optimizer='rmsprop',\r\n",
        "                  loss={'encoder_output': 'categorical_crossentropy', 'decoder_output': 'mean_squared_error'},\r\n",
        "                  loss_weights={'encoder_output': 0.5, 'decoder_output': 0.5},\r\n",
        "                  metrics=[metrics.categorical_accuracy])\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmKm-WgAYTva"
      },
      "source": [
        "## Designing an Auto-Encoder-Classifier model\r\n",
        "def Deep_SRC_Decoder(input_shape =600):\r\n",
        "    # Encoder Network\r\n",
        "    dec_input = Input(shape=(input_shape,), name='decoder_input')\r\n",
        "\r\n",
        "    # Decoder Network\r\n",
        "    dec_l1 = Dense(100, activation='relu', name='decoder_layer1')(dec_input)\r\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\r\n",
        "    dec_l1 = Dropout(rate = 0.3)(dec_l1)\r\n",
        "\r\n",
        "    dec_l2 = Dense(100, activation='relu', name='decoder_layer2')(dec_l1)\r\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\r\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\r\n",
        "\r\n",
        "    dec_l3 = Dense(100, activation='relu', name='decoder_layer3')(dec_l2)\r\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\r\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\r\n",
        "\r\n",
        "    dec_l4 = Dense(100, activation='relu', name='decoder_layer4')(dec_l3)\r\n",
        "    dec_l4 = BatchNormalization()(dec_l4)\r\n",
        "    dec_l4 = Dropout(rate = 0.3)(dec_l4)\r\n",
        "\r\n",
        "    decoder_output = Dense(840, activation='sigmoid', name='decoder_output')(dec_l4)\r\n",
        "\r\n",
        "    model = Model(inputs=[dec_input], outputs=[decoder_output])\r\n",
        "\r\n",
        "    # Compiling model\r\n",
        "    model.compile(optimizer='adam',#optimizer='rmsprop',\r\n",
        "                  loss={'decoder_output': 'mean_squared_error'},                  \r\n",
        "                  metrics=[metrics.mean_squared_error])\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M679bns3u7p7"
      },
      "source": [
        "## Define CKSAAP feature-extraction function\r\n",
        "def minSequenceLength(fastas):\r\n",
        "\tminLen = 10000\r\n",
        "\tfor i in fastas:\r\n",
        "\t\tif minLen > len(i[1]):\r\n",
        "\t\t\tminLen = len(i[1])\r\n",
        "\treturn minLen\r\n",
        "\r\n",
        "def CKSAAP(fastas, gap=5, **kw):\r\n",
        "\tif gap < 0:\r\n",
        "\t\tprint('Error: the gap should be equal or greater than zero' + '\\n\\n')\r\n",
        "\t\treturn 0\r\n",
        "\r\n",
        "\tif minSequenceLength(fastas) < gap+2:\r\n",
        "\t\tprint('Error: all the sequence length should be larger than the (gap value) + 2 = ' + str(gap+2) + '\\n\\n')\r\n",
        "\t\treturn 0\r\n",
        "\r\n",
        "\tAA = 'ACDEFGHIKLMNPQRSTVWY'\r\n",
        "\tencodings = []\r\n",
        "\taaPairs = []\r\n",
        "\tfor aa1 in AA:\r\n",
        "\t\tfor aa2 in AA:\r\n",
        "\t\t\taaPairs.append(aa1 + aa2)\r\n",
        "\theader = ['#']\r\n",
        "\tfor g in range(gap+1):\r\n",
        "\t\tfor aa in aaPairs:\r\n",
        "\t\t\theader.append(aa + '.gap' + str(g))\r\n",
        "\tencodings.append(header)\r\n",
        "\tfor i in fastas:\r\n",
        "\t\tname, sequence = i[0], i[1]\r\n",
        "\t\tcode = [name]\r\n",
        "\t\tfor g in range(gap+1):\r\n",
        "\t\t\tmyDict = {}\r\n",
        "\t\t\tfor pair in aaPairs:\r\n",
        "\t\t\t\tmyDict[pair] = 0\r\n",
        "\t\t\tsum = 0\r\n",
        "\t\t\tfor index1 in range(len(sequence)):\r\n",
        "\t\t\t\tindex2 = index1 + g + 1\r\n",
        "\t\t\t\tif index1 < len(sequence) and index2 < len(sequence) and sequence[index1] in AA and sequence[index2] in AA:\r\n",
        "\t\t\t\t\tmyDict[sequence[index1] + sequence[index2]] = myDict[sequence[index1] + sequence[index2]] + 1\r\n",
        "\t\t\t\t\tsum = sum + 1\r\n",
        "\t\t\tfor pair in aaPairs:\r\n",
        "\t\t\t\tcode.append(myDict[pair] / sum)\r\n",
        "\t\tencodings.append(code)\r\n",
        "\treturn encodings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yZMgda6FGMm"
      },
      "source": [
        "train_set = pd.read_csv(\"https://raw.githubusercontent.com/Shujaat123/AFP-SRC/master/data/train1.csv\")\r\n",
        "test_set = pd.read_csv(\"https://raw.githubusercontent.com/Shujaat123/AFP-SRC/master/data/test1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua12za40GXE6"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\r\n",
        "X_train = train_set.iloc[:, 1:].to_numpy()\r\n",
        "y_train = np.asarray(train_set.CLASS)\r\n",
        "y_train[y_train=='AFP']=1\r\n",
        "y_train[y_train=='NON_AFP']=0\r\n",
        "y_train = to_categorical(y_train)\r\n",
        "\r\n",
        "X_test = test_set.iloc[:, 1:].to_numpy()\r\n",
        "y_test = np.asarray(test_set.CLASS)\r\n",
        "y_test[y_test=='AFP']=1\r\n",
        "y_test[y_test=='NON_AFP']=0\r\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NjNTBe-vk39"
      },
      "source": [
        "# ## Loading and pre-processing AFP prediction dataset\r\n",
        "# data_path = 'https://raw.githubusercontent.com/Shujaat123/AFP-LSE/master/Dataset.csv'\r\n",
        "# dataset = pd.read_csv(data_path, index_col=None)\r\n",
        "\r\n",
        "# seq=[]\r\n",
        "\r\n",
        "# for index, row in dataset.iterrows():\r\n",
        "#   array = [row['Class'], row['Sequence']]\r\n",
        "#   name, sequence = array[0].split()[0], re.sub('[^ARNDCQEGHILKMFPSTWYV-]', '-', ''.join(array[1:]).upper())\r\n",
        "#   seq.append([name, sequence])\r\n",
        "\r\n",
        "\r\n",
        "# cksaapfea = []\r\n",
        "# for i in seq:\r\n",
        "#   temp= CKSAAP([i], gap=2)\r\n",
        "#   cksaapfea.append(temp)\r\n",
        "\r\n",
        "# dt = []\r\n",
        "# for i in range(len(cksaapfea)):\r\n",
        "#   temp = cksaapfea[i][1][1:]\r\n",
        "#   dt.append(temp)\r\n",
        "\r\n",
        "# dtn = np.array(dt)\r\n",
        "# from keras.utils.np_utils import to_categorical\r\n",
        "# label = dataset['Class']\r\n",
        "# # y = np.zeros((label.shape[0],600))\r\n",
        "# # y[label=='AFP',:]=to_categorical((np.random.randint(low = 0, high = 299, size = 1)),600)\r\n",
        "# # y[label=='NON-AFP',:]==to_categorical((np.random.randint(low = 300, high = 599, size = 1)),600)\r\n",
        "# label[label=='AFP']=1\r\n",
        "# label[label=='NON-AFP']=0\r\n",
        "# # y = to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCE3y5rlv0bV"
      },
      "source": [
        "# ## Train Test Split \r\n",
        "# plist = list(np.asarray(np.where(label==1)).flatten())\r\n",
        "# nlist = list(np.asarray(np.where(label==0)).flatten())\r\n",
        "\r\n",
        "# ## train\r\n",
        "# p_train = sample(plist, 300)\r\n",
        "# n_train = sample(nlist, 300)\r\n",
        "# train_list = p_train + n_train\r\n",
        "# X_train = dtn[p_train + n_train]\r\n",
        "# y_train = label[p_train + n_train]\r\n",
        "\r\n",
        "# ## valid\r\n",
        "# p_val_list = set(plist) - set(p_train)\r\n",
        "# n_val_list = set(nlist) - set(n_train)\r\n",
        "# X_test = dtn[list(p_val_list) + list(n_val_list)]\r\n",
        "# y_test = label[list(p_val_list) + list(n_val_list)]\r\n",
        "\r\n",
        "# # p_val = sample(p_val_list, 30)\r\n",
        "# # n_val = sample(n_val_list, 30)\r\n",
        "# # X_val = dtn[list(p_val)+list(n_val)]\r\n",
        "# # y_val = label[list(p_val)+list(n_val)]\r\n",
        "# # val_list = list(p_val) + list(n_val)\r\n",
        "\r\n",
        "# # ## test\r\n",
        "# # dev_list = train_list + val_list\r\n",
        "# # test_list = set(list(np.where(label)[0])) - (set(dev_list))\r\n",
        "# # X_test = dtn[list(test_list)]\r\n",
        "# # y_test = label[list(test_list)]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLhyrZXbvitu"
      },
      "source": [
        "def sample_one_hot_encoder(label):\r\n",
        "  ntrain = len(y_train)\r\n",
        "  onehot_encoded = list()\r\n",
        "  for value in range(ntrain):\r\n",
        "    letter = [0 for _ in range(ntrain)]\r\n",
        "    letter[value] = 1\r\n",
        "    onehot_encoded.append(letter)\r\n",
        "    \r\n",
        "  return np.array(onehot_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HbJtcuKxABo",
        "outputId": "4e488a7f-7149-4969-d7cf-365edd3a9a3e"
      },
      "source": [
        "y_train_latent = sample_one_hot_encoder(y_train)\r\n",
        "y_train_latent.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkGc8BX_32Wq"
      },
      "source": [
        "import numpy.linalg as LA\r\n",
        "def SRC_Pred(y_latent_pred):\r\n",
        "    y_pred=[]\r\n",
        "    for i in range(y_latent_pred.shape[0]):\r\n",
        "      \r\n",
        "        if (LA.norm(y_latent_pred[i][y_train.argmax(axis=1)==1])>= LA.norm(y_latent_pred[i][y_train.argmax(axis=1)==0])):\r\n",
        "            y_pred.append(1)\r\n",
        "        else:\r\n",
        "            y_pred.append(0)\r\n",
        "      \r\n",
        "    return to_categorical(np.array(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwsXsaBcMBOz",
        "outputId": "095cb910-505c-4d2a-8af1-2b0eac9fca92"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9372, 840)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL2lNsS1zqmf"
      },
      "source": [
        "# y_train_latent_pred.shape, y_test_latent_pred.shape\r\n",
        "# y_train_pred = SRC_Pred_Dict_Recon(y_train, y_train_latent_pred, X_train, X_train)\r\n",
        "# y_test_pred = SRC_Pred_Dict_Recon(y_test, y_test_latent_pred, X_train, X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fjpqSTKGWBB"
      },
      "source": [
        "def SRC_Pred_Dict_Recon(y_train, y_latent_pred, X_train, X_test):\r\n",
        "  npos = (y_train.argmax(axis=1)==1).sum()\r\n",
        "  nneg = len(y_train) - npos\r\n",
        "  y_latent_pred_pos = []\r\n",
        "  y_latent_pred_neg = []\r\n",
        "  for i in range(y_latent_pred.shape[0]):\r\n",
        "     y_latent_pred_pos.append(np.concatenate((np.array(K.softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==1], np.repeat(0,nneg)), axis=0))\r\n",
        "     y_latent_pred_neg.append(np.concatenate((np.repeat(0,npos), np.array(K.softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==1]), axis=0))\r\n",
        "\r\n",
        "  X_pred_pos = np.matmul(np.array(y_latent_pred_pos), X_train)\r\n",
        "  X_pred_neg = np.matmul(np.array(y_latent_pred_neg), X_train)\r\n",
        "  \r\n",
        "  y_pred=[]\r\n",
        "  for i in range(X_test.shape[0]):\r\n",
        "    pos_err = np.square(X_test[i] - X_pred_pos).sum()\r\n",
        "    neg_err = np.square(X_test[i] - X_pred_neg).sum()\r\n",
        "    if pos_err <= neg_err:\r\n",
        "      y_pred.append(1)\r\n",
        "    else:\r\n",
        "      y_pred.append(0)\r\n",
        "  \r\n",
        "  return to_categorical(np.array(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkZEW7TIJ_QM"
      },
      "source": [
        "def SRC_Pred_Dec_Recon(y_train, y_latent_pred, model, X_test, argmax=False):\r\n",
        "  npos = (y_train.argmax(axis=1)==1).sum()\r\n",
        "  nneg = len(y_train) - npos\r\n",
        "  y_latent_pred_pos = []\r\n",
        "  y_latent_pred_neg = []\r\n",
        "  for i in range(y_latent_pred.shape[0]):\r\n",
        "    if argmax:\r\n",
        "      tmp_y_latent_pred_pos = np.repeat(0, len(y_latent_pred[i]))\r\n",
        "      tmp_y_latent_pred_pos[y_latent_pred[i][y_train==1].argmax(axis=1)] = 1\r\n",
        "      tmp_y_latent_pred_neg = np.repeat(0, len(y_latent_pred[i]))\r\n",
        "      tmp_y_latent_pred_neg[y_latent_pred[i][y_train==0].argmax(axis=1)] = 1\r\n",
        "      y_latent_pred_pos.append(tmp_y_latent_pred_pos)\r\n",
        "      y_latent_pred_neg.append(tmp_y_latent_pred_neg)\r\n",
        "    else:\r\n",
        "     y_latent_pred_pos.append(np.concatenate((np.array(K.softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==1], np.repeat(0,nneg)), axis=0))\r\n",
        "     y_latent_pred_neg.append(np.concatenate((np.repeat(0,npos), np.array(K.softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==0]), axis=0))\r\n",
        "\r\n",
        "  y_latent_pred_pos = np.array(y_latent_pred_pos)\r\n",
        "  y_latent_pred_neg = np.array(y_latent_pred_neg)\r\n",
        "  X_pred_pos = model_dec.predict(y_latent_pred_pos)\r\n",
        "  X_pred_neg = model_dec.predict(y_latent_pred_neg) \r\n",
        "  \r\n",
        "  y_pred=[]\r\n",
        "  for i in range(X_test.shape[0]):\r\n",
        "    pos_err = np.square(X_test[i] - X_pred_pos[i]).sum()\r\n",
        "    neg_err = np.square(X_test[i] - X_pred_neg[i]).sum()\r\n",
        "    if pos_err <= neg_err:\r\n",
        "      y_pred.append(1)\r\n",
        "    else:\r\n",
        "      y_pred.append(0)\r\n",
        "  \r\n",
        "  return to_categorical(np.array(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sZOgQnV2Mfx"
      },
      "source": [
        "y_latent_pred = np.random.rand(len(y_train),600)\r\n",
        "y_pred = SRC_Pred(y_latent_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeh-L3Jp7icZ"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, balanced_accuracy_score, precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ6O1q4Y8VFh"
      },
      "source": [
        "from keras import backend as K\r\n",
        "from keras.models import load_model\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwjTfOFszbhv",
        "outputId": "7cc238b8-2fc5-466a-e487-61b6b274af58"
      },
      "source": [
        "model = Deep_SRC(input_shape = X_train.shape[1], LV=len(y_train))\r\n",
        "#model = Deep_VAE_SRC(input_shape = X_train.shape[1], LV=len(y_train))\r\n",
        "es = EarlyStopping(monitor='encoder_output_categorical_accuracy', mode='max', verbose=0, patience=100)\r\n",
        "checkpoint = ModelCheckpoint('models\\\\model-best.h5',\r\n",
        "                             verbose=0, monitor='encoder_output_categorical_accuracy',save_best_only=True, mode='auto')\r\n",
        "\r\n",
        "history = model.fit({'enc_input': X_train},\r\n",
        "                    {'encoder_output': y_train_latent, 'decoder_output': X_train},\r\n",
        "                    # validation_data = ({'enc_input': X_val},\r\n",
        "                    # {'class_output': y_val, 'decoder_output': X_val}),\r\n",
        "                    epochs=5000, batch_size=420, callbacks=[checkpoint, es], verbose=0)\r\n",
        "\r\n",
        "y_train_latent_pred, X_train_pred = model.predict(X_train,batch_size=540, verbose=0)\r\n",
        "#y_val_latent_pred, X_val_pred = model.predict(X_val,batch_size=540, verbose=0)\r\n",
        "y_test_latent_pred, X_test_pred = model.predict(X_test,batch_size=540, verbose=0)\r\n",
        "\r\n",
        "#del model\r\n",
        "#model = load_model('models\\\\model-best.h5')\r\n",
        "\r\n",
        "model_dec = Deep_SRC_Decoder()\r\n",
        "\r\n",
        "# es = EarlyStopping(monitor='loss', mode='min', verbose=0, patience=100)\r\n",
        "# checkpoint = ModelCheckpoint('models\\\\model_dec-best.h5',\r\n",
        "#                              verbose=0, monitor='loss',save_best_only=True, mode='auto')\r\n",
        "\r\n",
        "# history = model_dec.fit({'decoder_input': y_train_latent},\r\n",
        "#                     {'decoder_output': X_train},\r\n",
        "#                     # validation_data = ({'enc_input': X_val},\r\n",
        "#                     # {'class_output': y_val, 'decoder_output': X_val}),\r\n",
        "#                     epochs=5000, batch_size=600, callbacks=[checkpoint, es], verbose=0)\r\n",
        "# del model_dec\r\n",
        "# model_dec = load_model('models\\\\model_dec-best.h5')\r\n",
        "\r\n",
        "for i in range(1,12):\r\n",
        "    model_dec.layers[i].set_weights(model.layers[13+i].get_weights())\r\n",
        "\r\n",
        "## By Norm\r\n",
        "y_train_src_pred = SRC_Pred(y_train_latent_pred)\r\n",
        "y_test_src_pred = SRC_Pred(y_test_latent_pred)\r\n",
        "train_src_pred_acc = accuracy_score(y_train, y_train_src_pred)\r\n",
        "test_src_pred_acc = accuracy_score(y_test, y_test_src_pred)\r\n",
        "\r\n",
        "## By Dictionary (Train Dataset)\r\n",
        "y_train_src_dict_pred = SRC_Pred_Dict_Recon(y_train, y_train_latent_pred, X_train, X_train)\r\n",
        "y_test_src_dict_pred = SRC_Pred_Dict_Recon(y_train, y_test_latent_pred, X_train, X_test)\r\n",
        "train_src_dict_acc = accuracy_score(y_train, y_train_src_dict_pred)\r\n",
        "test_src_dict_acc = accuracy_score(y_test, y_test_src_dict_pred)\r\n",
        "\r\n",
        "## By Reconstruction (Model)\r\n",
        "y_train_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_train_latent_pred, model_dec, X_train)\r\n",
        "y_test_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_test_latent_pred, model_dec, X_test)\r\n",
        "#y_train_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_train_latent_pred, model_dec, X_train, argmax=True)\r\n",
        "#y_test_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_test_latent_pred, model_dec, X_test, argmax=True)\r\n",
        "train_src_rec_acc = accuracy_score(y_train,y_train_src_rec_pred)\r\n",
        "test_src_rec_acc = accuracy_score(y_test,y_test_src_rec_pred)\r\n",
        "\r\n",
        "print(\"train_acc_by_Norm: {}, test_acc_by_Norm: {}\".format(train_src_pred_acc, test_src_pred_acc))\r\n",
        "print(\"train_acc_by_Dict: {}, test_acc_by_Dict: {}\".format(train_src_dict_acc, test_src_dict_acc))\r\n",
        "print(\"train_acc_by_Recon: {}, test_acc_by_Recon: {}\".format(train_src_rec_acc, test_src_rec_acc))\r\n",
        "\r\n",
        "## Input Train loss\r\n",
        "MSE_X_train_pred = -10*np.log10(np.mean(np.square(X_train_pred - X_train)))\r\n",
        "#MSE_X_val_pred = -10*np.log10(np.mean(np.square(X_val_pred - X_val)))\r\n",
        "MSE_X_test_pred = -10*np.log10(np.mean(np.square(X_test_pred - X_test)))\r\n",
        "\r\n",
        "## Sample Label loss\r\n",
        "MSE_y_latent_train_pred = -10*np.log10(np.mean(np.square(y_train_latent_pred - y_train_latent)))\r\n",
        "\r\n",
        "print(MSE_X_train_pred, MSE_y_latent_train_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_acc_by_Norm: 0.5283333333333333, test_acc_by_Norm: 0.7170294494238156\n",
            "train_acc_by_Dict: 0.8116666666666666, test_acc_by_Dict: 0.8451771233461375\n",
            "train_acc_by_Recon: 0.5, test_acc_by_Recon: 0.01931284677763551\n",
            "4.196110474795418 27.749817264122036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz3Q97vju6Ho"
      },
      "source": [
        "def yoden_index(y, y_pred):\r\n",
        "  epsilon = 1e-30\r\n",
        "  tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\r\n",
        "  j = (tp/(tp + fn + epsilon)) + (tn/(tn+fp + epsilon)) - 1\r\n",
        "  return j\r\n",
        "\r\n",
        "def pmeasure(y, y_pred):\r\n",
        "    epsilon = 1e-30\r\n",
        "    tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\r\n",
        "    sensitivity = tp / (tp + fn + epsilon)\r\n",
        "    specificity = tn / (tn + fp + epsilon)\r\n",
        "    f1score = (2 * tp) / (2 * tp + fp + fn + epsilon)\r\n",
        "    return ({'Sensitivity': sensitivity, 'Specificity': specificity, 'F1-Score': f1score})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDk5IAHJs3f4"
      },
      "source": [
        "def Calculate_Stats(y_actual,y_pred):\r\n",
        "  acc = accuracy_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\r\n",
        "  sen = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['Sensitivity']\r\n",
        "  spe = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['Specificity']\r\n",
        "  f1 = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['F1-Score']\r\n",
        "  mcc = matthews_corrcoef(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\r\n",
        "  bacc = balanced_accuracy_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\r\n",
        "  yi = yoden_index(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\r\n",
        "  #auc = roc_auc_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\r\n",
        "  \r\n",
        "  #pre, rec, _ = precision_recall_curve(y_actual.argmax(axis=1), y_score, pos_label=1)\r\n",
        "  #fpr, tpr, _ = roc_curve(y_actual.argmax(axis=1), y_score, pos_label=1)\r\n",
        "  #auroc = auc(fpr, tpr)\r\n",
        "  #aupr = auc(rec, pre)\r\n",
        "\r\n",
        "  return acc, sen, spe, f1, mcc, bacc, yi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urNobQ7KuKF0"
      },
      "source": [
        "train_list = [y_train_src_pred, y_train_src_dict_pred, y_train_src_rec_pred]\r\n",
        "test_list = [y_test_src_pred, y_test_src_dict_pred, y_test_src_rec_pred]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3JRklKvsgnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283a0a53-9715-4e69-9747-f0809d845cd9"
      },
      "source": [
        "Stats=[]\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "  y_train_pred = train_list[i]\r\n",
        "  y_test_pred = test_list[i]\r\n",
        "  \r\n",
        "  ## Training Measures\r\n",
        "  tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi = Calculate_Stats(y_train, y_train_pred);\r\n",
        "  \r\n",
        "  ## Validation Measures\r\n",
        "  #v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi = Calculate_Stats(to_categorical(y_val),y_val_pred);\r\n",
        "  \r\n",
        "  ## Test Measures\r\n",
        "  t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi = Calculate_Stats(y_test,y_test_pred);\r\n",
        "\r\n",
        "  Stats.append([tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi,\r\n",
        "                #              v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi,\r\n",
        "                t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi])\r\n",
        "\r\n",
        "Statistics = np.asarray(Stats)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaCNG--RtbIx"
      },
      "source": [
        "def Show_Statistics(msg,mean_Stats, sd_Stats, sigfig):\r\n",
        "  print(msg.upper())\r\n",
        "  print(70*'-')\r\n",
        "  print('Accuracy:{} + {}'          .format(round(mean_Stats[0],sigfig), round(sd_Stats[0],sigfig)))\r\n",
        "  print('Sensitivity:{} + {} '      .format(round(mean_Stats[1],sigfig), round(sd_Stats[1],sigfig)))\r\n",
        "  print('Specificity:{} + {}'       .format(round(mean_Stats[2],sigfig), round(sd_Stats[2],sigfig)))\r\n",
        "  print('F1-Score:{} + {}'          .format(round(mean_Stats[3],sigfig), round(sd_Stats[3],sigfig)))\r\n",
        "  print('MCC:{} + {}'               .format(round(mean_Stats[4],sigfig), round(sd_Stats[4],sigfig)))\r\n",
        "  print('Balance Accuracy:{} + {}'  .format(round(mean_Stats[5],sigfig), round(sd_Stats[5],sigfig)))\r\n",
        "  print('Youden-Index:{} + {}'      .format(round(mean_Stats[6],sigfig), round(sd_Stats[6],sigfig)))\r\n",
        "  print(70*'-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uvCv738thYf",
        "outputId": "0df14b0e-c68f-4022-ece8-ef5ce3465beb"
      },
      "source": [
        "Show_Statistics('Norm Training Results (MEAN)',Statistics[0][0:7],Statistics.std(axis=0)[0:7], 3)\r\n",
        "Show_Statistics('Norm Test Results (MEAN)',Statistics[0][7:14],Statistics.std(axis=0)[7:14], 3)\r\n",
        "Show_Statistics('Dict Training Results (MEAN)',Statistics[1][0:7],Statistics.std(axis=0)[0:7], 3)\r\n",
        "Show_Statistics('Dict Test Results (MEAN)',Statistics[1][7:14],Statistics.std(axis=0)[7:14], 3)\r\n",
        "Show_Statistics('Rec Training Results (MEAN)',Statistics[2][0:7],Statistics.std(axis=0)[0:7], 3)\r\n",
        "Show_Statistics('Rec Test Results (MEAN)',Statistics[2][7:14],Statistics.std(axis=0)[7:14], 3)\r\n",
        "#Show_Statistics('Test Results (MEAN)',Statistics.mean(axis=0)[14:21],Statistics.std(axis=0)[14:21], 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NORM TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.528 + 0.141\n",
            "Sensitivity:0.357 + 0.267 \n",
            "Specificity:0.7 + 0.368\n",
            "F1-Score:0.431 + 0.155\n",
            "MCC:0.06 + 0.281\n",
            "Balance Accuracy:0.528 + 0.141\n",
            "Youden-Index:0.057 + 0.281\n",
            "----------------------------------------------------------------------\n",
            "NORM TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.717 + 0.363\n",
            "Sensitivity:0.37 + 0.263 \n",
            "Specificity:0.724 + 0.373\n",
            "F1-Score:0.048 + 0.058\n",
            "MCC:0.029 + 0.107\n",
            "Balance Accuracy:0.547 + 0.143\n",
            "Youden-Index:0.094 + 0.285\n",
            "----------------------------------------------------------------------\n",
            "DICT TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.812 + 0.141\n",
            "Sensitivity:0.78 + 0.267 \n",
            "Specificity:0.843 + 0.368\n",
            "F1-Score:0.806 + 0.155\n",
            "MCC:0.625 + 0.281\n",
            "Balance Accuracy:0.812 + 0.141\n",
            "Youden-Index:0.623 + 0.281\n",
            "----------------------------------------------------------------------\n",
            "DICT TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.845 + 0.363\n",
            "Sensitivity:0.801 + 0.263 \n",
            "Specificity:0.846 + 0.373\n",
            "F1-Score:0.167 + 0.058\n",
            "MCC:0.239 + 0.107\n",
            "Balance Accuracy:0.824 + 0.143\n",
            "Youden-Index:0.647 + 0.285\n",
            "----------------------------------------------------------------------\n",
            "REC TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.5 + 0.141\n",
            "Sensitivity:1.0 + 0.267 \n",
            "Specificity:0.0 + 0.368\n",
            "F1-Score:0.667 + 0.155\n",
            "MCC:0.0 + 0.281\n",
            "Balance Accuracy:0.5 + 0.141\n",
            "Youden-Index:0.0 + 0.281\n",
            "----------------------------------------------------------------------\n",
            "REC TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.019 + 0.363\n",
            "Sensitivity:1.0 + 0.263 \n",
            "Specificity:0.0 + 0.373\n",
            "F1-Score:0.038 + 0.058\n",
            "MCC:0.0 + 0.107\n",
            "Balance Accuracy:0.5 + 0.143\n",
            "Youden-Index:0.0 + 0.285\n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}